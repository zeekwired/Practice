{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T00:37:27.673943Z",
     "start_time": "2023-02-08T00:37:23.956393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T00:38:18.806508Z",
     "start_time": "2023-02-08T00:37:27.674910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "# View the shape of the dataset\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T00:38:19.024782Z",
     "start_time": "2023-02-08T00:38:18.808503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting features\n",
    "X = mnist.data\n",
    "# Setting target\n",
    "y = mnist.target\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a model on train and test sets\n",
    "    \"\"\"\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    print('Classification report: \\n', classification_report(y_test, model.predict(X_test)))\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T00:38:19.039672Z",
     "start_time": "2023-02-08T00:38:19.026778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "# Creating PCA object\n",
    "pca = PCA(n_components=.95)\n",
    "# Creating pipeline for StandardScaler and PCA\n",
    "transformer = make_pipeline(scaler, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T00:38:28.701999Z",
     "start_time": "2023-02-08T00:38:19.041668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zachd\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[3. 2. 8. ... 1. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Fitting first KNN model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m knn1\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> 6\u001b[0m evaluate(knn1, X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m      5\u001b[0m train_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[0;32m      6\u001b[0m test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain accuracy: \u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m.\u001b[39;49mscore(y_train, train_pred))\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mClassification report: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, classification_report(y_test, model\u001b[39m.\u001b[39mpredict(X_test)))\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(confusion_matrix(y_test, model\u001b[39m.\u001b[39mpredict(X_test)))\n",
      "File \u001b[1;32mc:\\Users\\zachd\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\pipeline.py:695\u001b[0m, in \u001b[0;36mPipeline.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    693\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    694\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 695\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    696\u001b[0m score_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zachd\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\pipeline.py:635\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    633\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    634\u001b[0m \u001b[39mfor\u001b[39;00m _, _, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[1;32m--> 635\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    636\u001b[0m \u001b[39mreturn\u001b[39;00m Xt\n",
      "File \u001b[1;32mc:\\Users\\zachd\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    972\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    974\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m--> 975\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    976\u001b[0m     X,\n\u001b[0;32m    977\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    978\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    979\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    980\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    981\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    982\u001b[0m )\n\u001b[0;32m    984\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\zachd\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\zachd\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[3. 2. 8. ... 1. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Creating first KNN model with PCA pipeline\n",
    "knn1 = make_pipeline(transformer, KNeighborsClassifier())\n",
    "# Fitting first KNN model\n",
    "knn1.fit(X_train, y_train)\n",
    "\n",
    "evaluate(knn1, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For this assignment, VSCode doesn't have the %%time function since the jupyter notebook extension comes with execution time built into the cell as default (Time took to execute: 28.2 sec). I commented it out for you so that you can run it and see the result on your end. \n",
    "> \n",
    "> - I know that jupyter notebook is required but I have dojo-env loaded and anaconda. Also, I like VSCode using my computers resources (they tend to be faster than client-server IDE's). So trying to be honest I'm going to be stubborn about switching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T00:38:55.030195Z",
     "start_time": "2023-02-08T00:38:33.125482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      1714\n",
      "           1       0.95      0.99      0.97      1977\n",
      "           2       0.95      0.93      0.94      1761\n",
      "           3       0.93      0.94      0.94      1806\n",
      "           4       0.94      0.93      0.94      1587\n",
      "           5       0.94      0.93      0.94      1607\n",
      "           6       0.96      0.97      0.97      1761\n",
      "           7       0.94      0.93      0.93      1878\n",
      "           8       0.97      0.89      0.93      1657\n",
      "           9       0.90      0.92      0.91      1752\n",
      "\n",
      "    accuracy                           0.94     17500\n",
      "   macro avg       0.94      0.94      0.94     17500\n",
      "weighted avg       0.94      0.94      0.94     17500\n",
      "\n",
      "[[1681    1    5    2    0    8   14    2    1    0]\n",
      " [   0 1962    8    1    2    0    1    1    1    1]\n",
      " [  18   23 1646   25    7    6   13   11    8    4]\n",
      " [   2    6   21 1703    3   16    2   26   14   13]\n",
      " [   0   16   14    1 1480    1    5    7    3   60]\n",
      " [   8    4    2   41    9 1498   22    3   11    9]\n",
      " [  14    4    7    0    8    9 1716    0    3    0]\n",
      " [   2   22    9    2   24    1    0 1747    1   70]\n",
      " [  14   22   16   31   10   49    7   13 1477   18]\n",
      " [   9    6    9   18   33    3    0   54    6 1614]]\n"
     ]
    }
   ],
   "source": [
    "# Creating second KNN model without PCA pipeline\n",
    "knn2 = make_pipeline(scaler, KNeighborsClassifier())\n",
    "knn2.fit(X_train, y_train)\n",
    "\n",
    "evaluate(knn2, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### a. Which model performed the best on the test set?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### b. Which model was the fastest at making predictions?\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4b31512f92221b1229e52ed4f26b9fed1a2ab2027382c38a12d18717dccb8195"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
