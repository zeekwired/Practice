{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Why Dimensionality Reduction?\n",
    "---\n",
    "# Why Do We Need Dimensionality Reduction?\n",
    "\n",
    "When working with big data, we often work with data that has a lot of columns -- or a lot of dimensions. Machine learning is a great tool to use with high dimensional data because it can pick up on patterns in high dimensional spaces that would be impossible for humans to pick up on. However, challenges can arise when working with high dimensional data.\n",
    "\n",
    "The \"curse of dimensionality\" refers to a phenomenon that arises when working in increasingly higher dimensional spaces - as the number of features increases, the amount of data needed to accurately generalize a model to new data grows exponentially.\n",
    "\n",
    "Dimensionality reduction refers to any technique where we try to reduce the feature space we are working with. In general, there are two types of dimensionality reduction:\n",
    "\n",
    "    1. Feature Selection\n",
    "    2. Feature Extraction\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "You have already been doing feature selection! This is the process of selecting a subset of features to model on. For example, if you are building a model to predict the price of homes, you can choose to use only the top most important features in your final model and leave out the features that may not be important to the model.\n",
    "\n",
    "But what if you don't know which features to choose?\n",
    "## Feature Extraction\n",
    "\n",
    "Feature extraction refers to techniques where you take all of your features and combine them in certain ways to reduce them into lower dimensions.\n",
    "\n",
    "Some popular methods of feature extraction include:\n",
    "\n",
    "    - Principal Component Analysis (PCA)\n",
    "    - Linear Discriminant Analysis (LDA)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Learning Objectives:\n",
    "\n",
    "    - Explain generally how PCA works.\n",
    "    - State the use cases of PCA.\n",
    "    - State the assumptions of PCA.\n",
    "\n",
    "# PCA\n",
    "\n",
    "Principal Component Analysis, abbreviated as PCA, is one of the most common techniques used for dimensionality reduction.\n",
    "It is an unsupervised learning technique, meaning we do not use a target/y-vector for this technique! PCA is applied to our features and is used to reduce the dimensions of our dataset.\n",
    "\n",
    "PCA transforms our features into a new set of variables called Principal Components, which are linear combinations of all of our original features. The algorithm relies heavily on linear algebra topics and understanding the math behind it is beyond the scope of this course, but if you are interested in learning more about how the algorithm works behind the scenes, check out the optional sections at the end of this chapter.\n",
    "\n",
    "The Principal Components (PCs) are arranged in order of how much variability they explain: the first PC explains the most variance of our original data, while the last PC explains the least amount of variation of our original data. We choose a select number of PCs that cumulatively explain enough variance in our data yet still reduces the dimensionality of our feature space.\n",
    "# Uses of PCA\n",
    "\n",
    "PCA is useful for two main tasks:\n",
    "\n",
    "    - Visualization: if we have high-dimensional data that would be impossible to visualize, sometimes by transforming that data into a lower-dimensional space we can visualize things like clusters.\n",
    "    - Dimensionality reduction to improve model speed: we can use PCA to first reduce the dimensions of our data, then use the PCs for modeling.\n",
    "\n",
    "# Assumptions of PCA\n",
    "\n",
    "When doing PCA, we assume:\n",
    "\n",
    "    - There are linear relationships between our features.\n",
    "    - Importance is determined from variance, i.e. higher variance means it is more important.\n",
    "    - All of our original features have the same units (if they don't, you need to scale your data!).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for Data Visualization\n",
    "\n",
    "Learning Objectives:\n",
    "\n",
    "    - Use PCA to visualize high dimensional data.\n",
    "---\n",
    "**Note**: you can watch a video walkthrough of this code at the end of this module.\n",
    "# PCA for Data Visualization\n",
    "\n",
    "One common application of PCA is to transform our data into lower-dimensional spaces in order to visualize it, a task that is impossible to do when we are working with more than 3 dimensions.\n",
    "## Create Data Visualizations using PCA in Python\n",
    "\n",
    "Let's try this in Python!\n",
    "### 1. **Import Libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use here is from [this source](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/) and describes information about various types of iris flowers.\n",
    "\n",
    "Let's say we want to cluster the data into different flower varieties. We might want to visually check how many clusters would be reasonable. Notice that we have several dimensions here and it would be tough to visualize this data as-is (we could only visualize a few features at a time).\n",
    "\n",
    "We can use PCA to transform this data into less dimensions!\n",
    "### 2. **Load the Dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the Iris dataset from the UCI datasets\n",
    "url= 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "df = pd.read_csv(url, names = ['sepal length','sepal width','petal length','petal width','target'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data into X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X data\n",
    "X = df.drop(columns='target')\n",
    "# Encode our target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['target'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scale Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiat Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit and transform data.\n",
    "scaled_df = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate, fit & transform data using PCA\n",
    "pca = PCA(n_components=2)\n",
    "pcs = pca.fit_transform(scaled_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use Outputs of PCA to Visualize the Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 2 PCs\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(pcs[:,0], pcs[:,1], c = y)\n",
    "plt.title('Visualization of all of our data using the first two Principal Components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff33bb1d8c88f86dd74f1de65935fb5d737ee676ec7fada7be4c0855fdf81de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
